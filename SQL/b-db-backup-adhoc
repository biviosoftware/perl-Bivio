#!/bin/sh
# -*- text -*-
#
# $Id$
#
# b-db-backup-adhoc [db-login] [db-name] [backup-host] [keep-days]
#

DB_LOGIN=${1:-bivio/bivio}
DB_NAME=${2:-SOC}
BACKUP_HOST=${3:-s2b}
KEEP_DAYS=${4:-4}

HOST=`hostname`
HOST=`expr $HOST : '\([^\.]*\)\.' '|' $HOST`

. /etc/ssh2/ssh2_agent

[ -d /home/bkp ] || mkdir /home/bkp
cd /home/bkp
ts=$(date +%Y%m%d%H%M%S)
mkdir $ts
cd $ts

# Use a named pipe to compress data on the fly
# Note: Even compressed dumps can eventually reach the 2GB limit,
# at which point we need to split the output.
PIPE=./dump_out
mkfifo $PIPE

# DON'T backup lock_t, because it is temporary

# Files is first, because it takes the longest and happens "deepest"
# in the night.  Also indexing starts at 4a.
gzip --fast <$PIPE >$ts-realm-files.dmp.gz &
# Consistent
$ORACLE_HOME/bin/exp ${DB_LOGIN}@$DB_NAME grants=n compress=y indexes=n rows=y constraints=n statistics=none feedback=0 consistent=y file=$PIPE 'tables=(FILE_T)'
wait

# Mail and file_quota_t next, so as soon after files as possible.
gzip --fast <$PIPE >$ts-realm-mail.dmp.gz &
# Consistent
$ORACLE_HOME/bin/exp ${DB_LOGIN}@$DB_NAME grants=n compress=y indexes=n rows=y constraints=n statistics=none feedback=0 consistent=y file=$PIPE 'tables=(MAIL_T,FILE_QUOTA_T)'
wait

gzip --fast <$PIPE >$ts-realm-common.dmp.gz &
# Consistent
$ORACLE_HOME/bin/exp ${DB_LOGIN}@$DB_NAME grants=n compress=y indexes=n rows=y constraints=n statistics=none feedback=0 consistent=y file=$PIPE 'tables=(ADDRESS_T,BIVIO_BUCK_T,CLUB_T,CONNECT_SURVEY_T,DB_UPGRADE_T,EC_PAYMENT_T,EC_SUBSCRIPTION_T,EMAIL_T,JAPAN_SURVEY_T,PASSWORD_REQUEST_T,PHONE_T,PREFERENCES_T,REALM_DECOR_T,REALM_INVITE_T,REALM_OWNER_T,REALM_ROLE_T,REALM_USER_T,TAX_ID_T,USER_T,VISITOR_T)'
wait

gzip --fast <$PIPE >$ts-realm-acct.dmp.gz &
# Consistent
$ORACLE_HOME/bin/exp ${DB_LOGIN}@$DB_NAME grants=n compress=y indexes=n rows=y constraints=n statistics=none feedback=0 consistent=y file=$PIPE 'tables=(ACCOUNT_SYNC_T,ENTRY_T,EXPENSE_CATEGORY_T,EXPENSE_INFO_T,MEMBER_ALLOCATION_T,MEMBER_ENTRY_T,REALM_ACCOUNT_ENTRY_T,REALM_ACCOUNT_T,REALM_INSTRUMENT_ENTRY_T,REALM_INSTRUMENT_T,REALM_INSTRUMENT_VALUATION_T,REALM_TRANSACTION_T,TAX_1065_T,TAX_K1_T)'
wait

# Compress --fast, so we don't keep oracle waiting too long
gzip --fast <$PIPE >$ts-instrument.dmp.gz &
# INSTRUMENT_T is needed separately to allow a proper import of the two other dumps
# Don't need consistent=y, because we only update once per day
$ORACLE_HOME/bin/exp ${DB_LOGIN}@$DB_NAME grants=n compress=y indexes=n rows=y constraints=n statistics=none feedback=0 consistent=n file=$PIPE 'tables=(INSTRUMENT_T,INSTRUMENT_MERGER_SPINOFF_T,INSTRUMENT_VALUATION_T)'
wait

gzip --fast <$PIPE >$ts-csi.dmp.gz &
# Don't need consistent=y, because we only update once per day
$ORACLE_HOME/bin/exp ${DB_LOGIN}@$DB_NAME grants=n compress=y indexes=n rows=y constraints=n statistics=none feedback=0 consistent=n file=$PIPE 'tables=(CSI_DISTRIBUTION_T,CSI_FACT_SHEET_T,CSI_IMPORT_T,CSI_INSTRUMENT_T,CSI_MUTUAL_FUND_T,CSI_SPLIT_T,CSI_STOCK_PRICE_T)'
wait
rm $PIPE

# Return to /home/bkp
cd ..

if [ "$BACKUP_HOST" != "NONE" ]; then
    echo Transferring to $BACKUP_HOST
    ssh $BACKUP_HOST "[ -d /home/bkp/$DB_NAME ] || mkdir /home/bkp/$DB_NAME"
    scp -pr $ts ${BACKUP_HOST}:/home/bkp/$DB_NAME
fi

# Keep a few days worth of backup online
thin=`ls -dt [0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9] | tail +$KEEP_DAYS`
if [ "$thin" ]; then
    echo Thinning backups on $HOST, removing $thin
    rm -rf $thin
fi
